{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"background-color:#F5F5F5;\" width=\"100%\">\n",
    "<tr><td style=\"background-color:#F5F5F5;\"><img src=\"../images/logo.png\" width=\"150\" align='right'/></td></tr>     <tr><td>\n",
    "            <h2><center>Aprendizagem Automática em Engenharia Biomédica</center></h2>\n",
    "            <h3><center>1st Semester - 2024/2025</center></h3>\n",
    "            <h4><center>Universidade Nova de Lisboa - Faculdade de Ciências e Tecnologia</center></h4>\n",
    "</td></tr>\n",
    "    <tr><td><h2><b><center>Lab 5 - Linear Models</center></b></h2>\n",
    "    <h4><i><b><center>Linear, Polynomial and Logistic Regression</center></b></i></h4></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Models\n",
    "\n",
    "Linear models are one of the most simple ML techniques, used both for regression and classification. Depending on the use case, they can be trained without much effort.\n",
    "\n",
    "These models perform statistical computations to find the line that best fits the training data, to make future predictions.\n",
    "\n",
    "\n",
    "## 2. Linear Regression\n",
    "\n",
    "Linear regression is a regression model which fits the line function that minimizes the residuals, i. e. the difference between the prediction and the real values.\n",
    "\n",
    "<img src=https://blog.dataiku.com/hs-fs/hubfs/Willy%20Wonkas%20Candy.png width=\"500\">\n",
    "\n",
    "Ref: https://blog.dataiku.com/top-machine-learning-algorithms-how-they-work-in-plain-english-1\n",
    "\n",
    "A __Simple Linear Regression__ is a model that only uses one variable to estimate the values of the target. In this example, the Age is used as feature to estimate the money spent per week in the pharmacy. \n",
    "\n",
    "The regression line assumes the known shape:\n",
    "\n",
    "$$y = m*x + b$$\n",
    "$$\\text{\\$ spent per week} = m * Age + b$$\n",
    "\n",
    "On the other hand, a __Multiple Linear Regression__ is the model that fits a line in a multi-dimensional space, i.e. leveraging multiple variables.\n",
    "\n",
    "$$\\text{\\$ spent per week} = m_1 * Age + m_2 * Income + m_3 * Comorbidities + b$$\n",
    "\n",
    "$m_1$, $m_2$, $m_3$ and $b$ are the model parameters that weight (m) each feature and add the bias (b).\n",
    "\n",
    "### 2.1. The Medical Cost Estimation Dataset\n",
    "\n",
    "This dataset contains a set of features related with the characteristics of individuals, which can be used to estimate the costs with medical insurance. This is a regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_insurance = pd.read_csv(\"Data/insurance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_insurance[df_insurance.columns[:-1]]\n",
    "y = df_insurance['charges']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "one_hot = ce.OneHotEncoder(cols=[\"region\"], use_cat_names=True, return_df=True, drop_invariant=True)\n",
    "one_hot.fit(X_train)\n",
    "X_train = one_hot.transform(X_train)\n",
    "X_test = one_hot.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "X_train[[\"sex\", \"smoker\"]] = encoder.fit_transform(X_train[[\"sex\", \"smoker\"]])\n",
    "X_test[[\"sex\", \"smoker\"]] = encoder.transform(X_test[[\"sex\", \"smoker\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "X_train[X_train.columns] = scaler_X.fit_transform(X_train) \n",
    "X_test[X_test.columns] = scaler_X.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_y = MinMaxScaler()\n",
    "y_train = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten() \n",
    "y_test = scaler_y.transform(y_test.values.reshape(-1, 1)) .flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Evaluating Regression Models\n",
    "\n",
    "Contrarily to classification models, which can be tested evaluating hits and misses, regression models must be treated differently. Therefore, other metrics that measure the error in a continuous scale should be used.\n",
    "\n",
    "For this purpose, it is useful to resort to the predictions' residuals for the evaluation, where we compare the estimated value to the actual one:\n",
    "\n",
    "$$ \\text{Residuals (error)} = y - \\hat{y}$$\n",
    "\n",
    "#### 2.2.1. Mean Absolute Error (MAE)\n",
    "\n",
    "MAE is obtained by calculating the absolute difference between the model predictions and the true values, giving a measure of the magnitude of the error. A model with MAE of zero is a perfect model.\n",
    "\n",
    "$$\\text{MAE} =  \\frac{1}{N}  \\sum_{i=1}^N  | y_i - \\hat{y}_i |$$\n",
    "\n",
    "\n",
    "#### 2.2.2. Mean Square Error (MSE)\n",
    "\n",
    "Similar to MAE, the MSE represents instead the squared difference between model prediction and true values. The errors are usually larger than MAE as the residuals are squared. Furthermore, from the quadratic behaviour, larger errors will be greatly penalized.\n",
    "\n",
    "$$\\text{MSE} =  \\frac{1}{N}  \\sum_{i=1}^N  ( y_i - \\hat{y}_i )^2$$\n",
    "\n",
    "\n",
    "#### 2.2.3. Root Mean Square Error (RMSE)\n",
    "\n",
    "The RMSE represents the standard deviation of the residuals. This metric is more easily interpreted compared to MSE as it matches the units of the output.\n",
    "\n",
    "RMSE provides an estimate of how large the residuals are being dispersed.\n",
    "\n",
    "\n",
    "$$\\text{RMSE} =  \\sqrt{\\frac{1}{N}  \\sum_{i=1}^N  ( y_i - \\hat{y}_i )^2}$$\n",
    "\n",
    "\n",
    "#### 2.2.4. Coefficient of Determination (R2)\n",
    "\n",
    "This metric evaluates the fit of the line to the dataset, thus providing a tool to compare results between different datasets. R2 represents the proportion of variance y that has been explained by the variables. In this case, the perfect model will have a R2 of one.\n",
    "\n",
    "$$\\text{R}^2 = \\frac{\\sum_{i=1}^N  ( y_i - \\hat{y}_i )^2}{\\sum_{t=1}^N  ( y_t - \\overline{y}_i )^2}$$\n",
    "\n",
    "\n",
    "### 2.3. Training a Linear Regression\n",
    "\n",
    "The training of a linear regression will find the values for the model parameters that minimize the line error.\n",
    "\n",
    "Linear regression models can be optimized using several algorithms, such as the _Least Squares estimation methods_, _Maximum-likelihood estimation_ or _Gradient Descent_.\n",
    "\n",
    "Here, we'll resort to the Least Squares estimation method to predict the money spent weekly resorting to the Age variable only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.scatter(X_train['age'].values, y_train, alpha=.2)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Medical Insurance Charges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the equation of the linear regression:\n",
    "\n",
    "$$\\hat{y} (x_i) = m*x_i + b$$\n",
    "\n",
    "Training our model means finding the appropriate parameters for m and b. Considering the residual of the prediction $i$ as $\\varepsilon_i$, we will minimize the total residual error, resorting to the cost function $J$:\n",
    "\n",
    "$$J(m, b) = \\frac{1}{2n} \\sum_{i=1}^{n} \\varepsilon^2_i$$\n",
    "\n",
    "Therefore, we must find $m$ and $b$ to which the cost function $J$ is minumum. For that purpose we'll resort to the following equations:\n",
    "\n",
    "$$m = \\frac{SS_{xy}}{SS_{xx}}\\text{,   } b = \\overline{y} - m \\overline{x}$$\n",
    "\n",
    "With $SS_{xy}$ the sum of cross-deviations of y and x, and $SS_{xx}$ the sum of squared deviations of x:\n",
    "\n",
    "$$SS_{xy} = \\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y}) = \\sum_{i=1}^n y_i x_i - n \\overline{x} \\overline{y}$$\n",
    "\n",
    "$$SS_{xx} = \\sum_{i=1}^n (x_i - \\overline{x})^2 = \\sum_{i=1}^n x_i^2 - n (\\overline{x})^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ref: https://www.geeksforgeeks.org/linear-regression-python-implementation/\n",
    "\n",
    "def estimate_coef(x, y):\n",
    "    # number of observations/points\n",
    "    n = np.size(x)\n",
    "  \n",
    "    # mean of x and y vector\n",
    "    m_x = np.mean(x)\n",
    "    m_y = np.mean(y)\n",
    "  \n",
    "    # calculating cross-deviation and deviation about x\n",
    "    SS_xy = np.sum(y*x) - n*m_y*m_x\n",
    "    SS_xx = np.sum(x*x) - n*m_x*m_x\n",
    "  \n",
    "    # calculating regression coefficients\n",
    "    m = SS_xy / SS_xx\n",
    "    b = m_y - m*m_x\n",
    "  \n",
    "    return (b, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating coefficients\n",
    "b, m = estimate_coef(X_train['age'].values, y_train)\n",
    "\n",
    "print(\"Estimated coefficients:\\nb = {} \\nm = {}\".format(b, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted response vector\n",
    "y_pred = b + m * X_test['age'].values\n",
    "\n",
    "# plotting the actual points as scatter plot\n",
    "plt.scatter(X_test['age'].values, y_test, alpha=.2)\n",
    "\n",
    "# plotting the regression line\n",
    "plt.plot(X_test['age'].values, y_pred, color = \"g\", lw=2)\n",
    "\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Medical Insurance Charges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the results of our regression (although we have already an idea...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "print(\"R2: {}\".format(r2_score(y_test, y_pred)))\n",
    "print(\"MAE: {}\".format(mean_absolute_error(y_test, y_pred)))\n",
    "print(\"MSE: {}\".format(mean_squared_error(y_test, y_pred)))\n",
    "print(\"RMSE: {}\".format(mean_squared_error(y_test, y_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What if we could resort to all features of the dataset?__\n",
    "\n",
    "\n",
    "### 2.4. Exercise\n",
    "\n",
    "__Exercise 1__: Using the Medical Insurance dataset, use all features to estimate a linear regression using Scikit-learn [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2__: Evaluate the results of this regression model. Does it improve? Could we accept it as model for production?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Polynomial Regression\n",
    "\n",
    "While the Linear Regression model has some relevance in the field thanks to its simplicity, most problems are not linearly separable. This problem often leads to poorly performing regressions.\n",
    "\n",
    "One way to address this is to resort to Polynomial Regression, which increases the order of magnitude of different variables. This allows to model a curve more fitted to the problem:\n",
    "\n",
    "$$\\hat{y} (x) = m_1*x_1 + m_2*x_1^2 + m_3*x_2 + m_4*x_2^2 + m_5*x_2^3 + b$$\n",
    "\n",
    "<img src=https://static.javatpoint.com/tutorial/machine-learning/images/machine-learning-polynomial-regression.png width=\"500\">\n",
    "\n",
    "### 3.1. Training a Polynomial Regression\n",
    "\n",
    "Scikit-learn doesn't have a specific estimator for the Polynomial Regression. Instead, it allows to apply transformations to the input features, increasing the order of magnitude of the variables. \n",
    "\n",
    "The fitted model, although leveraging polynomial features of various degrees, is still linear!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "poly = poly.fit(X_train)\n",
    "\n",
    "X_train_poly = pd.DataFrame(poly.transform(X_train), columns=poly.get_feature_names_out())\n",
    "X_testn_poly = pd.DataFrame(poly.transform(X_test), columns=poly.get_feature_names_out())\n",
    "\n",
    "X_train_poly.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Exercise\n",
    "\n",
    "__Exercise 3__: Train a new Linear Regression with the polynomial features. Evaluate the results and compare to the previous model. What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression\n",
    "\n",
    "Dispite its name, Logistic Regression is a linear model used for __classification__. It is also known as Logit regression, and is used to estimate the probability that an instance belongs to a class. If the prediction is higher than 50%, then the output will take the positive class.\n",
    "\n",
    "<img src=https://miro.medium.com/max/725/0*eMgJSdCcerlh__Ip.png width=\"600\">\n",
    "\n",
    "### 4.1. Training a Logistic Regression\n",
    "\n",
    "This algorithm resembles its linear models class, where the coefficients of a linear function are estimated towards minimizing a cost function. However, instead of outputing prediction values, it outputs the logistic of this result.\n",
    "\n",
    "$$\\hat{y} (x_i) = \\sigma{(m*x_i + b)}$$\n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1 + \\exp^{-t}}$$\n",
    "\n",
    "Let's chage the objective of the Medical Insurance dataset. Instead of estimating the annual charges a person is subject to, we'll try to predic whether that person smokes or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"charges\"] = y_train\n",
    "y_train = X_train[\"smoker\"]\n",
    "\n",
    "X_train.drop(\"smoker\", axis=1, inplace=True)\n",
    "\n",
    "X_test[\"charges\"] = y_test\n",
    "y_test = X_test[\"smoker\"]\n",
    "\n",
    "X_test.drop(\"smoker\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logr = LogisticRegression(random_state=0)\n",
    "logr = logr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Exercise\n",
    "\n",
    "__Exercise 4__: Evaluate the Logistic Regression model. Which metrics are more relevant for the task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ee3b8b0f13ce1b575702cf6f1b3dd3d8df18dc5f202e0bca1f4b2f664d388be6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"background-color:#F5F5F5;\" width=\"100%\">\n",
    "<tr><td style=\"background-color:#F5F5F5;\"><img src=\"../images/logo.png\" width=\"150\" align='right'/></td></tr>     <tr><td>\n",
    "            <h2><center>Aprendizagem Automática em Engenharia Biomédica</center></h2>\n",
    "            <h3><center>1st Semester - 2024/2025</center></h3>\n",
    "            <h4><center>Universidade Nova de Lisboa - Faculdade de Ciências e Tecnologia</center></h4>\n",
    "</td></tr>\n",
    "    <tr><td><h2><b><center>Lab 6 - Decision Trees and Feature Extraction</center></b></h2>\n",
    "    <h4><i><b><center>Human Activity Recognition with Decision Trees</center></b></i></h4></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Human Activity Recognition and Timeseries Data\n",
    "\n",
    "### 1.1 Human Activity Recognition\n",
    "Human Activity Recognition (HAR) is a growing field in machine learning where the goal is to classify physical activities based on data from inertal sensors (such as accelerometers, gyroscopes, magnetometers), video (RGB, depth or thermal cameras), audio (microphones), among others. Data collected from these sensors can be used to infer activities like walking, running, sitting, and more.\n",
    "\n",
    "HAR has a wide range of applications, such as:\n",
    "* __Healthcare__: monitoring patient activity and rehabilitation progress\n",
    "* __Fitness__: tracking exercise routines\n",
    "* __Smart Homes__: detecting daily activities for automation\n",
    "\n",
    "Since such activities change continuously, it is necessary to consider data collected throughout time, so one can predict the performed activity at any given moment.\n",
    "\n",
    "### 1.2 Timeseries Data\n",
    "Sensor data for HAR is inherently timeseries data, meaning that the data points are collected over time. This creates a sequence where each data point depends on the ones that precede it. Activities are continuous by nature, so understanding the temporal patterns is key to accurate recognition. For example, walking involves a repetitive cycle that can be detected by analyzing sequences of accelerometer readings.\n",
    "\n",
    "Describing it in mathematical terms, a timeseries is a sequence of data points indexed in time order:\n",
    "\n",
    "$$ \\{x_1, x_2, ..., x_n\\} $$\n",
    "\n",
    "where:\n",
    "\n",
    "* $n \\in \\mathbb{N}$ is the number of observations\n",
    "* $x_i$ is the observation at time $t_i$ for $i \\in \\{1, ..., n\\}$\n",
    "* $t_1 < t_2 <...<t_n$ represents the sequence of time points, with $t_i$ typically spaced at regular intervals.\n",
    "\n",
    "Each $x_i$ can be a scalar (single value) or a vector, depending on the number of variables being measured at each time point. The image below shows a discrete timeseries.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/hgamboa/nova-aaeb/refs/heads/main/images/timeseries.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "\n",
    "Working with timeseries data introduces unique challenges, such as:\n",
    "\n",
    "* __Sequential Dependency__: unlike traditional datasets where each sample is independent, timeseries data has an inherent temporal dependency between consecutive data points.\n",
    "* __High Dimensionality__: timeseries data can be very high-dimensional, especially when collected from multiple sensors over a long period.\n",
    "* __Windowing and Segmentation__: in order to convert continuous sensor streams into manageable data chunks for machine learning models, we need techniques like windowing.\n",
    "\n",
    "### 1.3 Goal of Today's Class\n",
    "In today's practical session, you will learn how to __develop Decision Tree Models for HAR__ by:\n",
    "\n",
    "* Using the UCI Human Activity Recognition Dataset.\n",
    "* Pre-processing (data preparation of) timeseries data by windowing it.\n",
    "* Extract features from the windowed data.\n",
    "* Train and evaluate Decision Trees.\n",
    "\n",
    "## 2.The UCI Human Activity Recognition Using Smartphones Dataset\n",
    "\n",
    "In today's class we are going to utlize a widely-used dataset for HAR: the [__UCI Human Activity Recognition Using Smartphones Dataset__](https://archive.ics.uci.edu/dataset/240/human+activity+recognition+using+smartphones):\n",
    "\n",
    "The UCI Human Activity Recognition Using Smartphones Dataset was collected from __30 participants__ (ageed between 19 and 48 years old) performing __six different activities__ while wearing a __smartphone on their waist__. The smartphone recorded data from its embedded accelerometer and gyroscope sensors at a constant rate of __50Hz__, capturing __three-dimensional linear acceleration and angular velocity__.\n",
    "\n",
    "The performed activites and their corresponding label are:\n",
    "\n",
    "| Activity             | Label  |\n",
    "|----------------------|--------|\n",
    "| Walking              | 1      |\n",
    "| Walking upstairs     | 2      |\n",
    "| Walking downstairs   | 3      |\n",
    "| Sitting              | 4      |\n",
    "| Standing             | 5      |\n",
    "| Laying               | 6      |\n",
    "\n",
    "\n",
    "The dataset is already dvidied into __train__ and __test__ sets. These are setup as follows:\n",
    "1. __Training Set__: 70% of the participants used for model training (a total of 21 pariticpants).\n",
    "2. __Test Set__: 30% of the participants used for model evaluation (a total of 9 participants).\n",
    "\n",
    "In this notebook, __we are only going to use the x-axis of the raw acceleration data__.\n",
    "\n",
    "### 2.1 Downloading the Dataset\n",
    "\n",
    "the code below will download the dataset into your project.\n",
    "\n",
    "_Note_: you might have to install the [__wget__](https://pypi.org/project/wget/) package in order to perfrom the dataset install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed for the Notebook\n",
    "# %matplotlib notebook\n",
    "# !pip install wget\n",
    "import os\n",
    "import wget\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Tuple, List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Download dataset from UCI repo -- #\n",
    "\n",
    "# check if dataset was already downloaded before\n",
    "if not os.path.exists(\"Data/UCI HAR Dataset.zip\"):\n",
    "\n",
    "    # define the url where the .zip file of the dataset is stored\n",
    "    url = 'https://archive.ics.uci.edu/static/public/240/human+activity+recognition+using+smartphones.zip'\n",
    "\n",
    "    # download the dataset\n",
    "    wget.download(url, out=\"Data/UCI HAR Dataset.zip\")\n",
    "\n",
    "# Unzip dataset\n",
    "zip_ref = zipfile.ZipFile(\"Data/UCI HAR Dataset.zip\", 'r')\n",
    "zip_ref.extractall(\"Data\")\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Getting an Understanding of the Structure of the Dataset\n",
    "\n",
    "The dataset that we downloaded contains several folder and files. As we are going to focus only the x-axis of the raw acceleration data, we will only need the following files and folders (all other files and folder can be ignored):\n",
    "\n",
    "* __activity_labels.txt__ (file): This file contains the labels for the performed activities.\n",
    "* __train__ (folder): This folder contains the data for training.\n",
    "* __test__ (folder): This folder contains the data for testing.\n",
    "\n",
    "Inside of the train and test folders we need the following file/folder:\n",
    "\n",
    "* __subject_train.txt__ (file): contains the IDs of the subjects corresponding to the sensor data used in the training set.\n",
    "* __subject_test.txt__ (file): contains the IDs of the subjects corresponding to the sensor data used in the testing set.\n",
    "* __y_train.txt__ (file): contains the activity labels corresponding to the sensor data used in the training set.\n",
    "* __y_test.txt__ (file): contains the activity labels corresponding to the sensor data for the testing data.\n",
    "* __Inertial Signals__ (folder): this folder contains the raw sensor files.\n",
    "\n",
    "Within the Inertial Signals folder, the only files that are relevant are:\n",
    "\n",
    "* __total_acc_x_train.txt__ (file): contains the raw accelerometer data of __all subjects__ used in the training set.\n",
    "* __total_acc_x_test.txt__ (file): contains the raw accelerometer data of __all subjects__ used in the testing set.\n",
    "\n",
    "### 2.3 Getting an Understanding of how the files correspond to each other\n",
    "\n",
    "To understand how all these files correspond to each other it first of all important to understand how the files containing the raw accelerometer data (total_acc_x_train.txt and total_acc_x_test.txt) are organized.\n",
    "\n",
    "The shape of both files is:\n",
    "* __total_acc_x_train.txt__: [7352, 128] (rows: 7352, columns: 128)\n",
    "* __total_acc_x_train.txt__: [2957, 128]\n",
    "\n",
    "Now you might wonder why the files have these shapes. The reason is that researchers who published this dataset, use a particular way to store the data. They divided the signals for each subject in to chunks (windows) containig 128 samples of data and then stored these data windows in each row of the dataset.\n",
    "\n",
    "With regrads to the other files they have the following shapes:\n",
    "* __subject_train.txt__: [7352, 1]\n",
    "* __subject_test.txt__: [2957, 1]\n",
    "* __y_train.txt__: [7352, 1]\n",
    "* __y_test.txt__: [2957, 1]\n",
    "\n",
    "This means, that for each row contained in the raw accelerometer data files we have the corresponding subject label. The correspondence between the files is visualized in the image below for the training data.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/hgamboa/nova-aaeb/refs/heads/main/images/UCI-Dataset.png\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Loading the data\n",
    "\n",
    "As described above, we will __only use the raw x-axis of the accelerometer sensor__. For your convenience we have already written a function that loads the data for you. The method, defined below performs the loading of one acceleration axis, including the original raw data and the corresponding labels. Furthermore, the ID of the subject that performed the activity is also loaded.\n",
    "\n",
    "_Note_: To reduce the amount of data to be loaded we __only load half (64) of the samples contained in each row__. This is only done, so that the time that it is needed to train the model is roughly reduced by half. In a real-world scenario we would use the entire data to train the model as the stacking of the reduced data introduces continuity errors.\n",
    "\n",
    "_Note_: In the function we are adjusting the labels and subject IDs so that we have a label and a subject ID for each data sample/time instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inertial_data(acc_axis: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Loads and processes the inertial signal data from the UCI Human Activity Recognition (HAR) dataset.\n",
    "\n",
    "    This function reads accelerometer data for a specified axis (e.g., 'x', 'y', or 'z') for both \n",
    "    the training and test datasets. It also loads corresponding activity labels and subject IDs. \n",
    "    The data is windowed and repeated 64 times to match the window size.\n",
    "\n",
    "    :param acc_axis: The axis of accelerometer data to load ('x', 'y', or 'z').\n",
    "\n",
    "    :returns: \n",
    "        - X_train: Training set accelerometer data (horizontally stacked).\n",
    "        - y_train: Repeated activity labels for the training set.\n",
    "        - subject_train: Repeated subject IDs for the training set.\n",
    "        - X_test: Test set accelerometer data (windowed and stacked horizontally).\n",
    "        - y_test: Repeated activity labels for the test set.\n",
    "        - subject_test: Repeated subject IDs for the test set.\n",
    "        - activity_labels: Array of activity labels for both training and test sets.\n",
    "\n",
    "    :rtype: Tuple of NumPy arrays containing processed data.\n",
    "    \"\"\"\n",
    "    # --- loading training data --- #\n",
    "    # Load training accelerometer data for the specified axis\n",
    "    X_train = np.loadtxt('Data/UCI HAR Dataset/train/Inertial Signals/total_{}_train.txt'.format(acc_axis))\n",
    "\n",
    "    # stack all data horizontally to obtain a 1D array\n",
    "    X_train = np.hstack([window[:64] for window in X_train])\n",
    "    \n",
    "    # Load activity labels for training data\n",
    "    y_train = np.loadtxt('Data/UCI HAR Dataset/train/y_train.txt', dtype=int)\n",
    "\n",
    "    # repeat 64 times to have a label for each training data sample\n",
    "    y_train = np.repeat(y_train, 64)\n",
    "    \n",
    "    # Load subject IDs for training data\n",
    "    subject_train = np.loadtxt('Data/UCI HAR Dataset/train/subject_train.txt', dtype=int)\n",
    "\n",
    "    # repeat 64 times to have a subject ID for each training data sample\n",
    "    subject_train = np.repeat(subject_train, 64)\n",
    "\n",
    "    # --- loading testing data --- #\n",
    "    # Load test accelerometer data for the specified axis\n",
    "    X_test = np.loadtxt('Data/UCI HAR Dataset/test/Inertial Signals/total_{}_test.txt'.format(acc_axis))\n",
    "\n",
    "    # stack all data horizontally to obtain a 1D array\n",
    "    X_test = np.hstack([window[:64] for window in X_test])\n",
    "    \n",
    "    # Load activity labels for test data\n",
    "    y_test = np.loadtxt('Data/UCI HAR Dataset/test/y_test.txt', dtype=int)\n",
    "\n",
    "    # repeat 64 times to have a label for each testing data sample\n",
    "    y_test = np.repeat(y_test, 64)\n",
    "    \n",
    "    # Load subject IDs for test data\n",
    "    subject_test = np.loadtxt('Data/UCI HAR Dataset/test/subject_test.txt', dtype=int)\n",
    "\n",
    "    # repeat 64 times to have a subject ID for each training data sample\n",
    "    subject_test = np.repeat(subject_test, 64)\n",
    "    \n",
    "    # Load activity labels (activity number and corresponding activity name)\n",
    "    activity_labels = pd.read_csv('Data/UCI HAR Dataset/activity_labels.txt', \n",
    "                                  index_col=0, header=None, sep=' ')\n",
    "    \n",
    "    # Return all the processed data\n",
    "    return (X_train, y_train, subject_train, X_test, y_test, subject_test, np.hstack(activity_labels.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data using the defined function and make some prints to see how many data samples we have for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "train_data, train_labels, subject_train, test_data, test_labels, subject_test, activity_labels = get_inertial_data('acc_x')\n",
    "\n",
    "# print out some information on the loaded data\n",
    "print(\"ACC train data shape: {} | target shape: {} | Subjects: {}\".format(train_data.shape, train_labels.shape, np.unique(subject_train).size))\n",
    "print(\"ACC test  data shape: {} | target shape: {} | Subjects: {}\".format(test_data.shape, test_labels.shape, np.unique(subject_test).size))\n",
    "print(\"\\nAvailable Activities:\\n{}\".format(activity_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image below shows how the _get_inertial_data()_ function retrieves the data from the UCI dataset and how it is organized in the end. The continuity errors are shown in red in the X_train timeseries.\n",
    "\n",
    "_Note_: The image is not to the correct scale. As it is hard to visualize each sample for a timeseries collected at 50 Hz, the image does not 100 % reflect reality. __In reality there exists a value for subject_train and y_train for each sample of X_train (i.e., for each time instance $t_i$ there is a triplet of values)__:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/hgamboa/nova-aaeb/refs/heads/main/images/HAR_dataset_organized.png\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 Visualizing the Data From One Subject\n",
    "\n",
    "Now that we loaded the data, we can visualize the data from one subject. The code below generates a plot of the accelerometer data and adds vertical lines at the transitons between the activities. We will plot the data of __subject 1__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the subject\n",
    "subject = 1\n",
    "\n",
    "# get the positions within the subject_train array that are equal to the subject\n",
    "# this creates a boolean array that can be used to index the train data\n",
    "pos_subject_1 = subject_train == subject\n",
    "\n",
    "# get the signal data that corresponds to subject 1\n",
    "signal_subject_1 = train_data[pos_subject_1]\n",
    "\n",
    "# get the labels that correspond to subject 1\n",
    "y_subject_1 = train_labels[pos_subject_1]\n",
    "\n",
    "# calculate the sequantial difference between the labels\n",
    "# this will show us where the transitions between activities are\n",
    "# example np.diff([1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]) | result: [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]\n",
    "activity_transitions = np.diff(y_subject_1)\n",
    "\n",
    "# get the indices of the transitions by checking where in the array there are values that are != 1\n",
    "idx_transitions = np.where(activity_transitions)[0]\n",
    "\n",
    "# get the target labels at the transition points\n",
    "y_transition = y_subject_1[idx_transitions]\n",
    "\n",
    "# insert the label at the end of the recording\n",
    "y_transition = np.append(y_transition, y_subject_1[-1])\n",
    "\n",
    "# get the corresponding activity labels (the string version) at the transition points \n",
    "transition_activity_labels = activity_labels[y_transition-1]\n",
    "\n",
    "# print the activity labels to see which activity was performed at what time\n",
    "print(\"the activity labels for the recording shown below are: \\n{}\".format(transition_activity_labels))\n",
    "\n",
    "\n",
    "# plot the data \n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(signal_subject_1) # plot the signal\n",
    "plt.vlines(idx_transitions , -.5, 2, color='orange') # plot the transition lines\n",
    "\n",
    "# add titles and labels\n",
    "plt.title(\"Acceleration Data from Subject {}\".format(subject))\n",
    "plt.xlabel('samples [n]')\n",
    "plt.ylabel('x_acc [m/s^2]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Checking the Distribution of Classes Within the Dataset\n",
    "\n",
    "As always, it is important to verify whether the dataset is balanced, as having class imbalance leads to suboptimal results. The code below plots the distrubtion of classes by visualizing the amount of data points we have for each activity class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(train_labels, range=[1,7], bins=6, width=0.8)\n",
    "plt.xticks(np.arange(1,7), activity_labels, rotation=45)\n",
    "\n",
    "# add titles and labels\n",
    "plt.title(\"Distribution of Classification Samples\")\n",
    "plt.xlabel(\"Activity Classes\")\n",
    "plt.ylabel(\"Number of Samples [n]\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Signal Windowing and Feature Extraction\n",
    "\n",
    "Time-series data, such as sensor data from accelerometers or gyroscopes, consists of continuous sequences of observations over time. Machine learning models, however, often expect fixed-size, independent samples. To make time-series data usable for such models, we need to transform these continuous sequences into shorter, fixed-length segments known as __windows__ to then later perform __feature extraction__ on these.\n",
    "\n",
    "The extracted features from each window are then the input to the model. Thorugh this process we can thus effectively, turn timeseries data into the usual format that is used to train machine learning models, where we have a vector of features for each instance we want to classify.\n",
    "\n",
    "The image below shows this process considering windowing without overlap.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/hgamboa/nova-aaeb/refs/heads/main/images/windowing_and_feature_extraction.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "### 3.1 Signal Windowing\n",
    "\n",
    "Windowing is essential because:\n",
    "\n",
    "* It allows us to capture patterns and information within smaller time segments, improving the model's ability to identify meaningful features.\n",
    "* By segmenting data into fixed-length windows, we standardize input shapes for model training.\n",
    "* Windowing also helps address challenges related to long dependencies and computational efficiency.\n",
    "\n",
    "In the HAR dataset, each activity is recorded over time, and dividing this into smaller, fixed windows allows models to learn patterns associated with specific actions over short intervals, e.g., walking or standing\n",
    "\n",
    "There exist two commonly used techniques for timeseries windowing. These are:\n",
    "\n",
    "1. __Non-overlapping Windows__:\n",
    "* Each window contains a fixed number of observations without overlap with adjacent windows.\n",
    "* This method is efficient but might miss transitions occurring at window boundaries.\n",
    "\n",
    "2. __Overlapping Winows__:\n",
    "* Overlapping windows allow each window to partially overlap with the previous one, often by a fixed number of time steps.\n",
    "* Overlapping helps retain information near the boundaries and may improve classification by increasing data availability, but it requires more computation\n",
    "* This technique also allows for increasing the data that is used for training and testing, thus it is particularly useful when there are less recordings.\n",
    "\n",
    "### 3.2 Choosing the Window Size\n",
    "Now one question remais: __how do we decide the size/length of our window?__\n",
    "\n",
    "Selecting the window size is crucial in time-series analysis as it influences the model's ability to capture relevant patterns in the data. Here are key factors to consider when deciding on an appropriate window size:\n",
    "\n",
    "1. __Nature of the Activity or Pattern__:\n",
    "* Shorter windows capture fine-grained details but may not include enough context for certain activities.\n",
    "*  Longer windows encompass more context, which is beneficial for identifying longer activities but may dilute short, transient patterns.\n",
    "* For human activity recognition (HAR), studies often use window sizes between 1 and 5 seconds, depending on the activity's complexity and variability.\n",
    "\n",
    "2. __Sampling Frequency__:\n",
    "* The dataset’s sampling rate (e.g., 50 Hz) influences how many observations fit into each window.\n",
    "* For instance, at 50 Hz, a 2-second window would contain $50 \\times 2 = 100$ time steps, whereas a 4-second window would contain 200.\n",
    "\n",
    "3. __Trade-off Between Model Performance and Efficiency__:\n",
    "\n",
    "* Larger windows produce fewer samples, which can improve computational efficiency but may reduce training data diversity.\n",
    "* Smaller windows yield more samples, which can improve performance but increase computational costs.\n",
    "\n",
    "4. __Overlapping and Boundary Conditions__:\n",
    "\n",
    "* Window size should balance well with stride length to capture boundary information, especially for activities that might transition near the edges of windows.\n",
    "\n",
    "As you can see, there is no standard way to define the window size. Thus you should always:\n",
    ">__Experiment with several window sizes and evaluate model performance to identify the optimal choice for a particular task.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.1__: \n",
    "\n",
    "Now that we have the basics covered, you will now implement a function that performs the windowing of the signal. For simplicty you will implement a __non-overlapping__ windowing scheme. You will use a __window size = 5 seconds__. \n",
    "\n",
    "Given that the data is now in a continuous format, you need to be careful whether your extracted window contains only data from a unique activity. Mixing data from different activities into your window will introduce noise into your training dataset. Thus, you need to find a strategy to avoid this.\n",
    "\n",
    "__Tasks__:\n",
    "\n",
    "* Implement a function the performs a non-overlapping windowing of the data for both X and y.\n",
    "* The function should be called: __window_splitter__.\n",
    "* The function should have the following parameters:\n",
    "    >* __acc__: timeseries acceleration data of shape (n_samples,) | Type: np.ndarray\n",
    "    >* __labels__: array of labels corresponding to each time step in acc, with shape (n_samples,) | Type: np.ndarray\n",
    "    >* __window_size__: desired window length in seconds. | Type: int\n",
    "    >* __fs__: the sampling frequency. | Type: int\n",
    "* The function should output:\n",
    "    >* __X_windows__: list containing the extracted windows. The list is of shape (number_of_windows, samples_per_window)\n",
    "    >* __y_windows__: list of labels assigned to each window. The list is of shape (number_of_windows)\n",
    "\n",
    "__Hints__:\n",
    "\n",
    "* Given that the window is given in seconds, you will have to calculate how many data samples fit into that window. This will make windowing the data a lot easier. YOu can calculate the number of samples that fit into a window using the following formula: $w_{samples} = w_{seconds} * f_s$.\n",
    "* The start index (i.e., the position where the last window starts) is: $index_{last window} = len(acc) - w_{samples} + 1$.\n",
    "* You can use the [range()](https://pythonbasics.org/range-function/) function to create a range object containing the all start indices of the windows. You can use this range object within a for-loop to cycle over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.2__: \n",
    "\n",
    "Now that the signal is windowed, it is time to extract features from each window. Again, to simplify you will only extract the following statistical features: \n",
    "* the __mean__ value of the window.\n",
    "* the __maximum__ value of the window.\n",
    "* the __minimum__ value of the window.\n",
    "\n",
    "__Tasks__:\n",
    "\n",
    "* Implement a function to calculate statistical features from the extracted windows.\n",
    "* The function should be called: __compute_statistical_features__.\n",
    "* The function should have the following parameter:\n",
    "    >* X_windows: list containing the extracted windows containing the accelerometer data. The list is of shape (number_of_windows, samples_per_window) | Type: List or np.ndarry\n",
    "* The functions should output:\n",
    "    >* statistical_features: A 2D array of shape (n_windows, 3), where each row contains the mean, max, and min of each window. | Type: list\n",
    "\n",
    "__Hints__:\n",
    "* You can use the functions [__numpy.mean()__](https://numpy.org/doc/stable/reference/generated/numpy.mean.html), [__numpy.min()__](https://numpy.org/doc/stable//reference/generated/numpy.min.html), and [__numpy.max()__](https://numpy.org/doc/stable//reference/generated/numpy.max.html). When you set a certain parameter, i.e., calculate along a certain dimension you can write the code without a for-loop.\n",
    "* The function [__numpy.column_stack()__](https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html) can help you in organizing your output into and array of shape (n_windows, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise: 3.3__: \n",
    "\n",
    "Now that you have written the functions for windowing the data and extract features from the windows, it is time to apply your functions to the data we loaded in section 2.4.\n",
    "\n",
    "__Tasks__:\n",
    "\n",
    "* Define the sampling frequency and the window size as constants. ($f_s = 50Hz$, w_{seconds} = 5s)\n",
    "* Use the developed functions to window the data and extract the features, for both the training and testing data.\n",
    "* Print the first 10 rows of training data after computing the statistical features to check whether your output is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decision Tree Classifier\n",
    "\n",
    "### 4.1 Decision Tree Basics\n",
    "Decision Tree is one of the most widely used ML algorithms. In this method, the learning function that maps a set of variables into the target is represented by a decision tree. Such trees can also be represented as sets of if-then rules for human readability. Decision trees have the power to map highly non-linear data.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/hgamboa/nova-aaeb/refs/heads/main/images/decision_tree.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "When looking at a decision tree, there are some important concepts to take into consideration:\n",
    "\n",
    "* __Root node__: The top node of a decision tree from which all node splits come from. It represents the entire population.\n",
    "\n",
    "* __Decision nodes__: These are nodes which are subdivided into new branches. Similarly, __parent nodes__ are those divided into sub-nodes, and these are called the __child nodes__.\n",
    "\n",
    "* __Leaf or Terminal nodes__: Includes all nodes that are not further subdivided, thus representing the predicted class.\n",
    "\n",
    "Decision Trees therefore classify samples by sorting them down from the root of the tree up to some leaf node, providing the final classification. Each node evaluates a specific attribute, which then divides the samples according to an optimized condition.\n",
    "\n",
    "These models were originally developed for classification, although variations exist for the regression task. By dividing the multidimensional space into sections, these models are quite robust to outliers.\n",
    "\n",
    "### 4.2 Optimizing Decision Trees\n",
    "\n",
    "Training a decision tree is an iterative process towards an optimized solution. In simple terms, at each iteration, every data feature is tested to assert which is the most useful to classify the examples.\n",
    "\n",
    "At every decision node, all features are tested, to select how the new branches will be created. The training continues using the training examples associated with each child node. The algorithm converges when no more samples are left to split, or when some stopping criteria is met.\n",
    "\n",
    "##### 4.2.1 Entropy\n",
    "\n",
    "One of the most common concepts related to the selection of decision features is _Entropy_. This measure characterizes the impurity of an arbitrary collection of examples, namely the set of samples available in some node. \n",
    "\n",
    "Entropy thus describes the purity of a node. The lower the value of entropy, the higher the node's purity. An homogeneous node has an entropy of 0. Therefore, the training process will create new subdivisions to lower the impurity of the system.\n",
    "\n",
    "The entropy of a set of samples can be computed by the following equation:\n",
    "\n",
    "$$ \\text{Entropy} = - \\sum_{i=1}{p_i . \\log_2{p_i}} $$\n",
    "\n",
    "where $p_i$ is the probability of each class $i$ within the samples of the decision node. Entropy will be 1 when dealing with a binary classification problem the training set contains an equal proportion of both values, i.e. $p_i=0.5$. If all examples are from the same class, then the entropy will be 0.\n",
    "\n",
    "The training process that uses entropy as a splitting strategy is called __Information Gain__, which is determined by:\n",
    "\n",
    "$$ \\text{Information Gain} = 1 - \\text{Entropy}$$\n",
    "\n",
    "The variable selection process will then exhaustively test the Information Gain brought by each feature.\n",
    "\n",
    "##### 4.2.2 Gini Impurity\n",
    "\n",
    "Another strategy often used is the Gini Impurity, which leverages the Gini concept. It translates the probability of correctly labeling a randomly chosen element if it was randomly labeled according to the distribution of labels in the node. It can be determined by:\n",
    "\n",
    "$$ \\text{Gini} = \\sum_{i=1}{p_i^2} $$\n",
    "\n",
    "Therefore, the Gini Impurity is determined by:\n",
    "\n",
    "$$ \\text{Gini Impurity} = 1 - \\text{Gini}$$\n",
    "\n",
    "### 3.3 Training Decision Trees in Scikit-learn\n",
    "\n",
    "Using scikit-klearn package training a Decision Tree is straight forward as the package provides a class that performs the necessary tasks:\n",
    "\n",
    "* [__DecisionTreeClassifier()__](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "\n",
    "The hyperparameters of the Decision Tree can be set during the intialization of the class. The code below uses the Decision Tree with its default paramaters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# initialize the decsision tree model using default paramaters\n",
    "clf = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 4.1:__ It is time to train and evaluate the decisison tree model. This will be your next task.\n",
    "\n",
    "__Tasks__:\n",
    "* Train the classifier\n",
    "* Check its performance using accuracy for both train and test sets.\n",
    "* What can you observe for the training and testing accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write your answer here__: double click to edit the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Visualizing the Trained Decision Tree\n",
    "\n",
    "Scikit-learn also provides the option to visualize the trained Decision Tree. This allows us to undertsand the decision boundaries for each decision the Decision Tree made, which makes this model easy interpretable.\n",
    "\n",
    "To visualize the Decision Tree, we can use the function:\n",
    "* [__plot_tree()__](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# create a figure\n",
    "plt.figure(figsize=(18,10))\n",
    "\n",
    "# plot the decision tree\n",
    "plot_tree(clf, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Avoiding Overfitting in Decision Trees\n",
    "\n",
    "As we saw from the code above, the trained Decision Tree overfit to the training data. When using a Decision Tree with the default parameters, the tree will always perfectly fit to the data. This is inherent to decisision tree as any amount of data points can be perfectly separated using a complex form of if-else blocks.\n",
    "\n",
    "To avoid overfitting it is necessary to set the __hyperparameters__ of the tree. This process is generally called __hyperparameter tuning__ and in the particular case of Decision Trees it is also referred to as __pruning__. As it is not possible to test all combinations by hand, it is useful to resort to additional optimization techniques (we will learn about optimization techniques in the next lab class).\n",
    "\n",
    "The DecisionTreeClassifier has the following hyperparameters in scikit_learn:\n",
    "\n",
    "| Hyperparameter             | Type & Default                        | Description |\n",
    "|----------------------------|---------------------------------------|-------------|\n",
    "| __criterion__                | _str_, default=`\"gini\"`              | The function used to measure the quality of a split. Options are `\"gini\"` (Gini impurity) and `\"entropy\"` (information gain). |\n",
    "| __splitter__                 | _str_, default=`\"best\"`              | Strategy to choose the split at each node. `\"best\"` chooses the best split, \"random\"` chooses a random split. |\n",
    "| __max_depth__                | _int_, default=`None`                | The maximum depth of the tree. Limits depth to prevent overfitting. If `None`, nodes expand until all leaves are pure or contain fewer than __min_samples_split__ samples. |\n",
    "| __min_samples_split__        | _int_ or `float`, default=`2`        | Minimum number of samples required to split an internal node. If `float`, it represents a percentage of the total samples. |\n",
    "| __min_samples_leaf__         | _int_ or `float`, default=`1`        | Minimum number of samples required to be at a leaf node. Helps avoid leaf nodes with very few samples. |\n",
    "| __min_weight_fraction_leaf__ | _float_, default=`0.0`               | Minimum weighted fraction of samples required to be at a leaf node, helpful with sample weights to balance uneven data. |\n",
    "| __max_features__             | _int_, _float_, _str_, or `None`, default=`None` | Number of features to consider when looking for the best split. If `int`, exact number; if `float`, fraction of total features; if `\"auto\"`, `\"sqrt\"`, or `\"log2\"`, uses square root or log base 2 of features. |\n",
    "| __max_leaf_nodes__           | _int_, default=`None`                | Limits number of leaf nodes to control model complexity and prevent overfitting. |\n",
    "| __min_impurity_decrease__    | _float_, default=`0.0`               | Splits only if impurity decrease is greater than or equal to this value, aiding in pruning. |\n",
    "| __class_weight__             | _dict_, _list of dicts_, `\"balanced\"`, or `None`, default=`None` | Adjusts weight of each class for imbalanced data. `\"balanced\"` adjusts weights inversely to class frequencies. |\n",
    "| __ccp_alpha__                | _float_, default=`0.0`               | Complexity parameter for **Minimal Cost-Complexity Pruning**. Higher values prune more tree branches by penalizing complexity. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.2__: \n",
    "\n",
    "Let's now train a tree that avoids overfitting to the data by pruning it. For simplicity you will only set one hyperparameter: __max_depth__\n",
    "\n",
    "__Tasks__:\n",
    "* Train a new DecisionTreeClassifier model\n",
    "* Set __max_depth = 5__\n",
    "* Check its performance using accuracy for both train and test sets.\n",
    "* Plot the trained tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.4:__ \n",
    "\n",
    "To get an understanding how our model performs on the different classes, it is useful to look at the confusion matrix\n",
    "\n",
    "__Tasks___:\n",
    "* Plot the confusion matrix to understand the misclassifications of our model.\n",
    "\n",
    "__Hint__:\n",
    "* As you have learned before you can use [__CofnsuionMatrixDisplay.from_predicitons()__](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Automatic Feature Extraction with TSFEL\n",
    "\n",
    "Another method to extract useful information from data is to leverage automatic feature extraction tools, which can compute in an efficient way more informative representations of our data.\n",
    "\n",
    "For time series data, the Time Series Feature Extraction Library ([TSFEL](https://tsfel.readthedocs.io/en/latest/)) is able to extract features from the temporal, statistical and spectral domains. \n",
    "\n",
    "We will try this tool in the Human Activity Recognition Dataset, but first we need to install it. Run on your terminal:\n",
    "\n",
    "`pip install tsfel`\n",
    "\n",
    "Let's see how it works. The computation process takes some time, TSFEL will extract 259 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsfel\n",
    "\n",
    "# get all features available in tsfel\n",
    "cfg = tsfel.get_features_by_domain()\n",
    "\n",
    "# extract features for the training and testing data\n",
    "X_train_tsfel = tsfel.time_series_features_extractor(cfg, train_windows, fs=50)\n",
    "X_test_tsfel = tsfel.time_series_features_extractor(cfg, test_windows, fs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tsfel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Excercise 4.1__: \n",
    "\n",
    "Final exercise! Given the new feature sets, you have to train a new DecisionTreeClassifier and evaluate its performance.\n",
    "\n",
    "__Tasks__:\n",
    "\n",
    "* Train a new DecisionTreeClassifier model\n",
    "* Set __max_depth = 5__\n",
    "* Check its performance using accuracy for both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more features increase our accuracy! But now we have __250__ features instead of __3__. \n",
    "\n",
    "With this high number of features we can apply __feature selection methods__ and increase again our accuracy. In the next lab we will learn about how to implement these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ee3b8b0f13ce1b575702cf6f1b3dd3d8df18dc5f202e0bca1f4b2f664d388be6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"background-color:#F5F5F5;\" width=\"100%\">\n",
    "<tr><td style=\"background-color:#F5F5F5;\"><img src=\"../images/logo.png\" width=\"150\" align='right'/></td></tr>     <tr><td>\n",
    "            <h2><center>Aprendizagem Automática em Engenharia Biomédica</center></h2>\n",
    "            <h3><center>1st Semester - 2022/2023</center></h3>\n",
    "            <h4><center>Universidade Nova de Lisboa - Faculdade de Ciências e Tecnologia</center></h4>\n",
    "</td></tr>\n",
    "    <tr><td><h2><b><center>Lab 3 - Evaluation in ML</center></b></h2>\n",
    "    <h4><i><b><center>Metrics and Techniques</center></b></i></h4></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ML Challenges\n",
    "\n",
    "The development and training of ML models comprise a set of challenges that must be dealt with, to ensure usable robust solutions. Otherwise, although some initial results may seem promising, problems in when models are deployed in real-world will appear.\n",
    "\n",
    "Different challenges can be overcomed by different techniques, but we need to know them before we go deeper into the solutions. Here are some of the main problems:\n",
    "\n",
    "* __Insufficient Training Data__: Most ML models require a large set of examples to generalise well in even simple problems. Often we need thousands or millions of examples, however depending on the number of dimensions of the problem.\n",
    "\n",
    "* __Nonrepresentative Training Data__: Available data must be representative of all possible scenarios related to the problem. If training data does not characterize well the problem, it is expected that new data in production will be severely different.\n",
    "\n",
    "* __Poor-Quality Data__: The data collection process is often problematic, leading to examples with errors, outliers and noise. This makes it harder for the model to converge to the underlying patterns of the problem.\n",
    "\n",
    "* __Irrelevant Features__: There is a saying in ML world: _Garbage In, Garbage Out_. If we want the model to learn the problem, data variables must contain the most relevant information. Furthermore, if training data is full of irrelevant features, the model may not be able to converge to a good solution.\n",
    "\n",
    "* __Overfitting__: ML algorithms can often take quite complex forms, if they are allowed to, as training will always try to improve results. This search can end up with overfitted solution, which are behave seamlessly in training, but completely fails when new data comes.\n",
    "\n",
    "* __Underfitting__: The contrary may also happen, when a model is too simple to learn the underlying structure of the data. If the reality is more complex than the model, then predictions are doomed to fail, even in training.\n",
    "\n",
    "<h4><center>No Free Lunch Theorem</center></h4>\n",
    "<img src=https://media.makeameme.org/created/so-youre-telling-591c7f.jpg width=\"300\">\n",
    "\n",
    "<h3><center>How can we ensure we have trained a robust ML model?</center></h3>\n",
    "\n",
    "## 2. Model Evaluation\n",
    "\n",
    "Every ML model must be submitted to a set of evaluation strategies, to ensure that their performance meets the necessary requirements and does not fail in new testing data. \n",
    "\n",
    "During the development of a ML pipeline, the are a set of evaluation processes we can implement, to attest its quality:\n",
    "\n",
    "* __Train-Test Data Splits__\n",
    "\n",
    "* __Performance Metrics__\n",
    "\n",
    "* __Cross Validation__\n",
    "\n",
    "\n",
    "## 3. Scikit-Learn Package\n",
    "\n",
    "The [Scikit-learn](https://scikit-learn.org/stable/index.html) library is the standard tool to train end-to-end ML pipelines. It offers a set of tools that range from data preparation to model training and evaluation.\n",
    "\n",
    "Its methods are highly consistent between algorithms and provide an easy user interface:\n",
    "\n",
    "* __Estimators__: Includes all objects that can estimate some set of paramenter based on a training set. The process od estimation is performed by the `fit()` method. When calling for an estimator, one can define a set of additional parameters to guide the training/fitting process, which are called hyperparameters.\n",
    "\n",
    "* __Transformers__: Some estimators can also transform a dataset, especially with data preparation tools. These can be called by the `transform()` method, which resorts to previously fitted parameters to transform the data. The `fit_transform()` method is also available, to perform both actions in one method.\n",
    "\n",
    "* __Predictors__: On the other hand, some estimators can make predictions on new data, after being fitted on the training set. The new data is sent to the `predict()` method, which returns the predictions. One can also call the `score()` method, which measures the quality of the predictions on the test set.\n",
    "\n",
    "Also, `sklearn` is built upon existing libraries, such as `Numpy`, which allows to use common array procedures. \n",
    "\n",
    "### 3.1. Rescuing Titanic\n",
    "\n",
    "Let's recall our Titanic use case and use it as our working example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_titanic = pd.read_csv(\"Data/titanic.csv\")\n",
    "df_titanic.set_index('PassengerId', inplace=True)\n",
    "\n",
    "# Add family size feature\n",
    "df_titanic['Family'] = df_titanic[\"SibSp\"] + df_titanic[\"Parch\"]\n",
    "\n",
    "# Drop irrelevant features\n",
    "X_data = df_titanic.drop([\"Survived\", \"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "\n",
    "# Set target\n",
    "y_data = df_titanic[\"Survived\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, shuffle=False)\n",
    "\n",
    "# Transform Categorical Features\n",
    "import category_encoders as ce\n",
    "one_hot = ce.OneHotEncoder(cols=[\"Embarked\"], use_cat_names=True, handle_unknown='return_nan',\n",
    "                         handle_missing='return_nan', return_df=True, drop_invariant=True)\n",
    "one_hot.fit(X_train)\n",
    "X_train = one_hot.transform(X_train)\n",
    "X_test = one_hot.transform(X_test)\n",
    "\n",
    "# Transform Binary Features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "X_train[\"Sex\"] = encoder.fit_transform(X_train[\"Sex\"])\n",
    "X_test[\"Sex\"] = encoder.transform(X_test[\"Sex\"])\n",
    "\n",
    "# Missing values Imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median', copy=False)\n",
    "X_train.loc[:, :] = imp.fit_transform(X_train)\n",
    "X_test.loc[:, :] = imp.transform(X_test)\n",
    "\n",
    "# Min-Max Scaling \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Training a Classifier\n",
    "\n",
    "To study how we can evaluate a model, we firstly need to have a model. Let us train a Decision Tree for the survival prediction task.\n",
    "\n",
    "*Note*: We will not worry with the training process, for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='gini', max_depth=10, min_samples_split=25, random_state=0)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_pred_proba = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = y_test.to_frame()\n",
    "results['Predicted'] = y_test_pred\n",
    "results['Predicted Prob'] = y_test_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Exercices\n",
    "\n",
    "__Exercice 1__: Recall the Breast Cancer dataset and apply the previous data preparation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercice 2__: Train a straightforward [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) model on the Breast Cancer dataset.\n",
    "\n",
    "Hint: use default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Metrics\n",
    "\n",
    "The evaluation of a ML model may be done through several performance metrics. The choice of which ones should be used in each context is highly relevant. Different dataset characteristics demand for different metrics, as one can be mislead by not appropriate results.\n",
    "\n",
    "### 4.1. Important Terms\n",
    "\n",
    "Before diving into different performance metrics, we need to establish the definition of four key terms:\n",
    "\n",
    "* __True positives (TP)__: Predicted positive and are actually positive.\n",
    "\n",
    "* __False positives (FP)__: Predicted positive and are actually negative.\n",
    "\n",
    "* __True negatives (TN)__: Predicted negative and are actually negative.\n",
    "\n",
    "* __False negatives (FN)__: Predicted negative and are actually positive.\n",
    "\n",
    "### 4.2. Confusion Matrix\n",
    "\n",
    "Confusion matrices disclose different counts related to the number of times instances of class A are\n",
    "classified as class B. Basically, it provides a representation of the above terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Accuracy\n",
    "\n",
    "This metric compares the total number of correct predictions against the total. This is the most common metric, however it is not the best performance evaluator, especially in imbalanced datasets.\n",
    "\n",
    "$$Accuracy = {TP + TN \\over TP + TN + FP + FN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print('Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Precision or Positive Predictive Value (PPV)\n",
    "\n",
    "Precision denotes the percentage of predicted positive samples out of the total predicted positive samples. With this metric we can answer the question _How much the model is right when it says it is right?_.\n",
    "\n",
    "$$Precision = {TP\\over TP + FP}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "prec = precision_score(y_test, y_test_pred)\n",
    "\n",
    "print('Precision:', prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Recall, Sensitivity or True Positive Rate (TPR)\n",
    "\n",
    "This metric denotes the percentage of correctly positive predicted samples against the total positive samples. It allows to answer _How much the model is able to predict the positive cases?_.\n",
    "\n",
    "$$Recall = {TP\\over TP + FN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "\n",
    "print('Recall:', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Specificity or True Negative Rate (TNR)\n",
    "\n",
    "On the contrary, this metric details the percentage of correctly predicted negative examples among the total number of negative cases. We can then answer the question _How much the model is able to predict the negative cases?_.\n",
    "\n",
    "$$Specificity = {TN\\over TN + FP}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = tn / (tn + fp)\n",
    "\n",
    "print('Specificity:', spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = recall_score(y_test, y_test_pred, pos_label=0)\n",
    "\n",
    "print('Specificity:', spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. F1 Score\n",
    "\n",
    "The F1 score is computed by the harmonic mean of precision and recall. This takes the contribution of both, so higher the F1 score, the better. \n",
    "\n",
    "Whereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values. As a result, the model will only get a high F1 score only if both recall and precision are\n",
    "high.\n",
    "\n",
    "$$\\text{F1 Score} = {2\\over {1 \\over Precision} + {1 \\over Recall}} = {2 * Precision * Recall\\over Precision + Pecall}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8. Precision-Recall Curve\n",
    "\n",
    "This curve evaluates the model by scoring based on different decision functions, or threshold values, when the predictions can be evaluated as probabilities.The PR curve than plots the balance between Precision and Recall values.\n",
    "\n",
    "This type of curve is more suited to compare between different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrecisionRecallDisplay.from_predictions(y_test, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9. Receiver Operating Characteristic (ROC) Curve\n",
    "\n",
    "The ROC curve also compares, for different threshold values, the True Positive Rate (TPR) against the False Positive Rate (FPR). This curve allows to evaluate the quality of the model in terms of its capacity to detect both real positive and negative cases.\n",
    "\n",
    "In these curves, the top left corner is the usually where the best threshold sits, if we give equal importance to Recall and Specificity.\n",
    "\n",
    "$$\\text{True Positive Rate  (TPR)} = Recall = {TP\\over TP + FN}$$\n",
    "\n",
    "$$\\text{False Positive Rate  (FPR)} = 1 - Specificity = {FP\\over TN + FP}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, RocCurveDisplay\n",
    "\n",
    "fprs, tprs, thresholds = roc_curve(y_test, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_predictions(y_test, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10. Area Under the ROC Curve (AUROC / ROC AUC)\n",
    "\n",
    "This metric translates the ROC curve of a model to a single numerical value, which allows for a more easy comparison between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auroc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "\n",
    "print('ROC AUC:', auroc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11. Exercices\n",
    "\n",
    "__Exercice 3__: For the Breast Cancer dataset and trained classifier, present the relevant performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercice 4__: Compare the Accuracy of Train and Test sets. What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercice 5__: Considering the use case and objective, which metric(s) become(s) more relevant? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercice 6__: Obtain the ROC curve values and plot. What do you conclude from the graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercice 7__: Which decisioning threshold is the most appropriate, considering equal relevance for TPR and FPR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ee3b8b0f13ce1b575702cf6f1b3dd3d8df18dc5f202e0bca1f4b2f664d388be6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
